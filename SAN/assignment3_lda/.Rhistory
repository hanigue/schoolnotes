wineData <- read.csv(file = "wine.csv")
View(wineData)
View(wineData)
averages <-  aggregate(x = wineData[,-1],                # Specify data column
by = list(wineData[1,]),              # Specify group indicator
FUN = mean)
averages <-  aggregate(x = wineData[,-1],                # Specify data column
by = list(wineData$X1),              # Specify group indicator
FUN = mean)
View(averages)
withinMatrix <- cov(as.matrix(data[[1]])) * (nrow(data[[1]])-1)
withinMatrix <- cov(as.matrix(data[[1]])) * (nrow(data[[1]])-1)
# 2) scatter matrices
#############  within-class scatter matrix Sw ##################
withinScatterMatrix <- ComputeWithinScatter(splittedData, nrow(table(labels)))
averages[[1]]
averages[1,]
averages[,1]
nrow(averages[[1]]-1)
nrow(averages[,1]-1)
data <- wineData
View(data)
View(data)
withinMatrix <- cov(as.matrix(data[[1]])) * (nrow(data[[1]])-1)
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
mydata <- mydata[,-1]
mydata <- read.csv("wine.csv", header = FALSE)
labels <- mydata[,1]
labels <- as.factor(labels)
mydata <- mydata[,-1]
labels
withinMatrix
averages[[1]]
averages[[2]]
averages[,2]
averages[,-1]
grandMean <- apply(classMean[,-1],2,mean)
# Class mean
classMean <- aggregate(x = wineData[,-1], # Specify data column
by = list(wineData$X1),             # Specify group indicator
FUN = mean)
grandMean <- apply(classMean[,-1],2,mean)
grandMean
# by = nad datama nejakyho faktoru provest funkcei man hihi
covMatrix <- by(wineData[,-1],wineData$X1,cov)
View(covMatrix)
View(covMatrix)
covMatrix
by(wineData,wineData$X1,nrows)
by(wineData,wineData$X1,nrow)
sum(covMatrix * (by(wineData,wineData$X1,nrows) - 1))
sum(covMatrix * (by(wineData,wineData$X1,nrow) - 1))
str(covMatrix)
str((by(wineData,wineData$X1,nrow) - 1)))
str((by(wineData,wineData$X1,nrow) - 1))
sum(covMatrix[,-1]*(by(wineData,wineData$X1,nrow) - 1))
sum(covMatrix[1,]*(by(wineData,wineData$X1,nrow) - 1))
sum(covMatrix$`1`*(by(wineData,wineData$X1,nrow) - 1))
View(covMatrix)
View(covMatrix)
by(wineData,wineData$X1,nrow)
numRow <- by(wineData,wineData$X1,nrow)
numRow$1
numRow$X1
numRow
str(numRow)
numRow[1]
numRow[2]
numRow[,-1]
numRow[2,-1]
numRow[2,0]
numRow[2,]
numRow[2]
numRow[[2]]
numRow[[1]]
sum(covMatrix$`1`*(numRow[[1]]- 1))
sum(covMatrix*(numRow[[1]]- 1))
sum(covMatrix[[1]]*(numRow[[1]]- 1))
sum(covMatrix[[1]]*(as.vector(numRow) - 1))
sum(covMatrix[[1]]*(as.vector(numRow -1)))
sum(as.matrix(covMatrix)*(as.vector(numRow) - 1))
sum(covMatrix[[]]*(as.vector(numRow) - 1))
sum(covMatrix[[,-1]]*(as.vector(numRow) - 1))
sum(covMatrix[,-1]*(as.vector(numRow) - 1))
sum(covMatrix[[-1]]*(as.vector(numRow) - 1))
lapply(covMatrix,function(x) x*as.vector(numRow) - 1)
mapply(function(x,y) x*y,covMatrix, numRow, SIMPLIFY=FALSE)
sum(mapply(function(x,y) x*y,covMatrix, numRow, SIMPLIFY=FALSE))
Reduce('+',mapply(function(x,y) x*y,covMatrix, numRow, SIMPLIFY=FALSE))
classMean
grandMean
classMean - grandMean
View(classMean)
View(classMean)
classMean[,-1]
classMean[,-1] - grandMean
matrix(1:3)
matrix(2:3)
matrix(1:3,1:3)
matrix(1:3,2:3)
matrix(c(1:3,2:3))
matrix(c(1:3,2))
matrix(c(1,2:3,2))
matrix(1:3, nrow = 3, ncol=3)
matrix(1:3, nrow = 3, ncol=3)^T
t(matrix(1:3, nrow = 3, ncol=3))
betweenMatrix <- t(numRow*(classMean[,-1] - grandMean)(classMean[,-1] - grandMean))
betweenMatrix <- numRow*(classMean[,-1] - grandMean)*t(classMean[,-1] - grandMean)
betweenMatrix
classMean
apply(betweenMatrix,2,sum)
apply(betweenMatrix,1,sum)
apply(betweenMatrix[,-1],2,sum)
Reduce('+',betweenMatrix,2)
Reduce('+',betweenMatrix)
colSums(betweenMatrix)
betweenMatrix
colSums(betweenMatrix[,-1])
betweenMatrix <- colSums(betweenMatrix)
rm(list=ls())
mydata <- read.csv("wine.csv", header = FALSE)
labels <- mydata[,1]
labels <- as.factor(labels)
mydata <- mydata[,-1]
#compute LDA and return corresponding eigenvectors
eigenLDA <- LDA(mydata, labels)
#Compute within-scatter matrix
ComputeWithinScatter <- function(data, n)
{
withinMatrix <- cov(as.matrix(data[[1]])) * (nrow(data[[1]])-1)
for (class in 2:n) {
withinMatrix <- withinMatrix + cov(as.matrix(data[[class]])) * (nrow(data[[class]])-1)
}
return(withinMatrix)
}
#Compute between-scatter matrix
ComputeBetweenScatter <- function(data, n, meanOverall)
{
betweenMatrix <- nrow(data[[1]]) * as.matrix((colMeans(data[[1]])-meanOverall)%*%t(colMeans(data[[1]])-meanOverall))
for (class in 2:n) {
betweenMatrix <- betweenMatrix + nrow(data[[class]]) * as.matrix((colMeans(data[[class]])-meanOverall)%*%t(colMeans(data[[class]])-meanOverall))
}
return(betweenMatrix)
}
#Solve the EigenProblem and return eigen-vector
SolveEigenProblem <- function(withinMatrix, betweenMatrix)
{
eivectors <- eigen(solve(withinMatrix) %*% betweenMatrix)
return(eivectors)
}
ComputeCentroids <- function(data, labels){
yGroupedMean <- aggregate(as.data.frame(data), by = list(labels), FUN = mean)
rownames(yGroupedMean) <- yGroupedMean[,1]
yGroupedMean <- yGroupedMean[,-1]
return(yGroupedMean)
}
Classify <- function(newData, eigenVectors, labels, centroids){
y <- as.matrix(newData) %*% eigenVectors[,1:(length(levels(labels))-1)]
prior <- table(labels)/sum(table(labels))
classification <- matrix(nrow = nrow(newData), ncol = length(levels(labels)))
colnames(classification) <- levels(labels)
for(c in levels(labels))
{
classification[,c] <- as.matrix(0.5*rowSums((y - matrix(rep(as.matrix(centroids[c,]),
nrow(newData)), nrow = nrow(newData),
byrow = TRUE) )^2)
- log(prior[c]))
}
return(levels(labels)[apply(classification, MARGIN = 1, which.min)])
}
CrossvalidationLDA <- function(mydata, labels, kfolds = 10){
set.seed(17)
#randomly shuffle the data
random <- sample(nrow(mydata))
data <-mydata[random,]
labels <- labels[random]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(data)),breaks=kfolds,labels=FALSE)
acc <- rep(0, times = kfolds)
#10 fold cross validation
for(i in 1:kfolds){
#Segment your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- data[testIndexes, ]
trainData <- data[-testIndexes, ]
testLabels <- labels[testIndexes]
trainLabels <- labels[-testIndexes]
eigenLDA <- LDA(trainData, trainLabels)
centroids <- ComputeCentroids(as.matrix(trainData) %*% eigenLDA[,1:(length(levels(trainLabels))-1)],
labels = trainLabels)
pre <- Classify(newData = testData, labels = trainLabels, eigenVectors = eigenLDA,
centroids = centroids)
acc[i] <- sum(pre == testLabels)/length(testLabels)
}
return(mean(acc))
}
LDA <- function(mydata, labels){
#number of classes
n <-length(levels(labels))
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
# 2) scatter matrices
#############  within-class scatter matrix Sw ##################
withinScatterMatrix <- ComputeWithinScatter(splittedData, nrow(table(labels)))
#############  between-class scatter matrix Sb ##################
betweenScatterMatrix <- ComputeBetweenScatter(splittedData, nrow(table(labels)), colMeans(as.matrix(mydata)))
# 3)  eigen problem
############ solve Eigen problem ################################
ei <- SolveEigenProblem(withinScatterMatrix, betweenScatterMatrix)
#transform the samples onto the new subspace
y <- (as.matrix(mydata) %*% ei$vectors[,1:2])
## visual comparison with PCA
par(mfrow=c(1,2))
pca <- prcomp(mydata, scale. = FALSE)
plot(y[,1], y[,2], col = labels, pch = 21, lwd = 2, xlab = "LD1" , ylab = "LD2", main = "LDA")
plot(-pca$x, col = labels, pch = 21, lwd = 2, main = "PCA")
return(ei$vectors)
}
#compute LDA and return corresponding eigenvectors
eigenLDA <- LDA(mydata, labels)
#find centroids in the transformed data
centroids <- ComputeCentroids(as.matrix(mydata) %*% eigenLDA[,1:(length(levels(labels))-1)],
labels = labels)
#make predictions on the "mydata"
prediction <- Classify(newData = mydata, labels = labels, eigenVectors = eigenLDA,
centroids = centroids)
#ACC
sum(prediction == labels)/(length(labels))
data
ComputeWithinScatter(mydata,3)
splitData <- split(mydata,labels)
ComputeWithinScatter(splitData,3)
ComputeBetweenScatter(splitData,3,colMeans(as.matrix(mydata)))
rm(list=ls())
# Class mean
classMean <- aggregate(x = wineData[,-1], # Specify data column
by = list(wineData$X1),             # Specify group indicator
FUN = mean)
wineData <- read.csv(file = "wine.csv")
# Class mean
classMean <- aggregate(x = wineData[,-1], # Specify data column
by = list(wineData$X1),             # Specify group indicator
FUN = mean)
# Grand mean
grandMean <- apply(classMean[,-1],2,mean)
# by = nad datama nejakyho faktoru provest funkcei man hihi
covMatrix <- by(wineData[,-1],wineData$X1,cov)
numRow <- by(wineData,wineData$X1,nrow)
sum(covMatrix*(as.vector(numRow) - 1))
withinMatrix <- Reduce('+',mapply(function(x,y) x*y,covMatrix, numRow, SIMPLIFY=FALSE))
View(wineData)
View(wineData)
View(wineData)
View(withinMatrix)
View(withinMatrix)
withinMatrix <- Reduce('+',mapply(function(x,y) x*y,covMatrix, numRow - 1, SIMPLIFY=FALSE))
View(withinMatrix)
View(withinMatrix)
mydata <- read.csv("wine.csv", header = FALSE)
labels <- mydata[,1]
labels <- as.factor(labels)
nrow(table(labels)
nrow(table(labels))
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
nrow(splittedData[[1]])-1
cov(as.matrix(splittedData[[1]]))
covMatrix[[1]]
mydata <- read.csv("wine.csv", header = FALSE)
labels <- mydata[,1]
labels <- as.factor(labels)
mydata <- mydata[,-1]
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
cov(as.matrix(splittedData[[1]]))
betweenMatrix <- numRow*(classMean[,-1] - grandMean)*t(classMean[,-1] - grandMean)
betweenMatrix <- colSums(betweenMatrix)
betweenMatrix
betweenMatrix <- numRow*(classMean[,-1] - grandMean)*t(classMean[,-1] - grandMean)
betweenMatrix
?%*%
?'%*%'
betweenMatrix <- numRow*(classMean[,-1] - grandMean)%*%t(classMean[,-1] - grandMean)
betweenMatrix <- numRow*(wineData[,-1] - grandMean)%*%t(wineData[,-1] - grandMean)
rm(list=ls())
ComputeWithinScatter <- function(data, n)
{
# Covariance matrix
covMatrix <- lapply(data,cov)
numRow <- as.vector(unlist(lapply(data,nrow)))
withinMatrix <- Reduce('+',mapply(function(x,y) x*(y-1),covMatrix,numRow, SIMPLIFY=FALSE))
return(withinMatrix)
}
ComputeBetweenScatter <- function(data, n, meanOverall)
{
# Compute class mean
classMean <- by(mydata, labels, colMeans)
# make it a matrix instead of 3 lists in a matrix
classMean <- matrix(unlist(classMean), ncol = ncol(mydata), byrow = TRUE)
numRow <- as.vector(unlist(lapply(data,nrow)))
diff <- list()
for (i in 1:n) {
diff[[i]] <- numRow[i] *
(classMean[i,] - meanOverall) %*% t(classMean[i,] - meanOverall)
}
betweenMatrix <- Reduce('+',diff)
return(betweenMatrix)
}
SolveEigenProblem <- function(withinMatrix, betweenMatrix, prior)
{
eivectors <- eigen(solve(withinMatrix) %*% betweenMatrix)
return(eivectors)
}
ComputeCentroids <- function(data, labels){
yGroupedMean <- aggregate(as.data.frame(data), by = list(labels), FUN = mean)
rownames(yGroupedMean) <- yGroupedMean[,1]
yGroupedMean <- yGroupedMean[,-1]
return(yGroupedMean)
}
Classify <- function(newData, eigenVectors, labels, centroids){
y <- as.matrix(newData) %*% eigenVectors[,1:(length(levels(labels))-1)]
prior <- table(labels)/sum(table(labels))
classification <- matrix(nrow = nrow(newData), ncol = length(levels(labels)))
colnames(classification) <- levels(labels)
for(c in levels(labels))
{
classification[,c] <- as.matrix(0.5*rowSums((y - matrix(rep(as.matrix(centroids[c,]),
nrow(newData)), nrow = nrow(newData),
byrow = TRUE) )^2)
- log(prior[c]))
}
return(levels(labels)[apply(classification, MARGIN = 1, which.min)])
}
CrossvalidationLDA <- function(mydata, labels, kfolds = 10){
set.seed(17)
#randomly shuffle the data
random <- sample(nrow(mydata))
data <-mydata[random,]
labels <- labels[random]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(data)),breaks=kfolds,labels=FALSE)
acc <- rep(0, times = kfolds)
#10 fold cross validation
for(i in 1:kfolds){
#Segment your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- data[testIndexes, ]
trainData <- data[-testIndexes, ]
testLabels <- labels[testIndexes]
trainLabels <- labels[-testIndexes]
eigenLDA <- LDA(trainData, trainLabels)
centroids <- ComputeCentroids(as.matrix(trainData) %*% eigenLDA[,1:(length(levels(trainLabels))-1)],
labels = trainLabels)
pre <- Classify(newData = testData, labels = trainLabels, eigenVectors = eigenLDA,
centroids = centroids)
acc[i] <- sum(pre == testLabels)/length(testLabels)
}
return(mean(acc))
}
LDA <- function(mydata, labels){
#number of classes
n <-length(levels(labels))
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
# 2) scatter matrices
#############  within-class scatter matrix Sw ##################
withinScatterMatrix <- ComputeWithinScatter(splittedData, n)
#############  between-class scatter matrix Sb ##################
betweenScatterMatrix <- ComputeBetweenScatter(splittedData, n, colMeans(mydata))
# 3)  eigen problem
############ solve Eigen problem ################################
ei <- SolveEigenProblem(withinScatterMatrix, betweenScatterMatrix)
#transform the samples onto the new subspace
y <- (as.matrix(mydata) %*% ei$vectors[,1:2])
## visual comparison with PCA
par(mfrow=c(1,2))
pca <- prcomp(mydata,scale. = FALSE)
plot(y[,1], y[,2], col = labels, pch = 21, lwd = 2, xlab = "LD1" , ylab = "LD2", main = "LDA")
plot(-pca$x, col = labels, pch = 21, lwd = 2, main = "PCA")
return(ei$vectors)
}
############################# FUNCTIONS END ###################################
############################# MAIN ##########################################
### PREPARE DATA
#data(iris)
#mydata <- iris
#labels <- mydata[,5]
#mydata <- mydata[,-5]
mydata <- read.csv("wine.csv", header = FALSE)
labels <- mydata[,1]
labels <- as.factor(labels)
mydata <- mydata[,-1]
#compute LDA and return corresponding eigenvectors
eigenLDA <- LDA(mydata, labels)
#find centroids in the transformed data
centroids <- ComputeCentroids(as.matrix(mydata) %*% eigenLDA[,1:(length(levels(labels))-1)],
labels = labels)
#make predictions on the "mydata"
prediction <- Classify(newData = mydata, labels = labels, eigenVectors = eigenLDA,
centroids = centroids)
#ACC
sum(prediction == labels)/(length(labels))
#CrossValidation
accLDA <- CrossvalidationLDA(mydata, labels, kfolds = 10)
## visual comparison with PCA
par(mfrow=c(1,2))
pca <- prcomp(mydata,scale. = TRUE)
plot(y[,1], y[,2], col = labels, pch = 21, lwd = 2, xlab = "LD1" , ylab = "LD2", main = "LDA")
#transform the samples onto the new subspace
y <- (as.matrix(mydata) %*% ei$vectors[,1:2])
LDA <- function(mydata, labels){
#number of classes
n <-length(levels(labels))
# 1) split the data w.r.t. given factors
splittedData <- split(mydata, labels)
# 2) scatter matrices
#############  within-class scatter matrix Sw ##################
withinScatterMatrix <- ComputeWithinScatter(splittedData, n)
#############  between-class scatter matrix Sb ##################
betweenScatterMatrix <- ComputeBetweenScatter(splittedData, n, colMeans(mydata))
# 3)  eigen problem
############ solve Eigen problem ################################
ei <- SolveEigenProblem(withinScatterMatrix, betweenScatterMatrix)
#transform the samples onto the new subspace
y <- (as.matrix(mydata) %*% ei$vectors[,1:2])
## visual comparison with PCA
par(mfrow=c(1,2))
pca <- prcomp(mydata,scale. = TRUE)
plot(y[,1], y[,2], col = labels, pch = 21, lwd = 2, xlab = "LD1" , ylab = "LD2", main = "LDA")
plot(-pca$x, col = labels, pch = 21, lwd = 2, main = "PCA")
return(ei$vectors)
}
#compute LDA and return corresponding eigenvectors
eigenLDA <- LDA(mydata, labels)
#find centroids in the transformed data
centroids <- ComputeCentroids(as.matrix(mydata) %*% eigenLDA[,1:(length(levels(labels))-1)],
labels = labels)
#make predictions on the "mydata"
prediction <- Classify(newData = mydata, labels = labels, eigenVectors = eigenLDA,
centroids = centroids)
#ACC
sum(prediction == labels)/(length(labels))
#CrossValidation
accLDA <- CrossvalidationLDA(mydata, labels, kfolds = 10)
